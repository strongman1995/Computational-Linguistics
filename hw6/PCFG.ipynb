{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_grammar(s):\n",
    "    \"\"\"\n",
    "    give a string, find all the grammar rules in the string\n",
    "    \"\"\"\n",
    "    grammar_repo = []\n",
    "    simple_grammars = re.findall(\"\\([^\\(\\)]*\\)\", s)\n",
    "    # repeat until you cannot find any ( lhs rhs ) grammar\n",
    "    while len(simple_grammars) > 0:\n",
    "        # add all simple grammar into grammar repository\n",
    "        # replace them with their head\n",
    "        for simple_grammar in simple_grammars:\n",
    "            grammar = simple_grammar.split(\" \")\n",
    "            # '(' == grammar[0] and ')' == grammar[-1]\n",
    "            lhs = grammar[1]\n",
    "            rhs = grammar[2:-1]\n",
    "            grammar_repo.append([lhs, rhs])\n",
    "\n",
    "            s = s.replace(simple_grammar, lhs)\n",
    "\n",
    "        simple_grammars = re.findall(\"\\([^\\(\\)]*\\)\", s)\n",
    "    return grammar_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = \"data/train_data.txt\"\n",
    "contents = []\n",
    "with open(train_file) as f:\n",
    "    for line in f:\n",
    "        contents.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_grammar = []\n",
    "for s in contents:\n",
    "    train_grammar += get_grammar(s)#从train data得到grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PCFG():\n",
    "    def __init__(self):\n",
    "        self.grammer = []\n",
    "        \n",
    "    def count_grammer(self,train_grammar):\n",
    "        # 删除train_grammar中重复的grammar\n",
    "        # 只留下unique grammar\n",
    "        g_count_list = []\n",
    "        for g in train_grammar:\n",
    "            if g not in self.grammer:\n",
    "                g_count = train_grammar.count(g)\n",
    "                g_count_list.append(g+[g_count])\n",
    "                self.grammer.append(g)\n",
    "        \n",
    "        # 计算grammar 左边non-terminals的频度\n",
    "        left_tokens = [g[0] for g in train_grammar]\n",
    "        left = {}\n",
    "        already = []\n",
    "        for token in left_tokens:\n",
    "            if token not in already:\n",
    "                left[token] = left_tokens.count(token)\n",
    "                already.append(token)\n",
    "        \n",
    "        # 计算grammar频率\n",
    "        for g in g_count_list:\n",
    "            g[2] = 1.0 * g[2]/left[g[0]]\n",
    "\n",
    "        # LHS相同的，按照频率从小到大排\n",
    "        sorted_g_list = []\n",
    "        for token in left.keys():\n",
    "            cur_g_list = []\n",
    "            for g in g_count_list:\n",
    "                if token == g[0]:\n",
    "                    cur_g_list.append(g)\n",
    "            cur_g_list = sorted(cur_g_list,key = lambda x:x[2])\n",
    "            sorted_g_list += cur_g_list\n",
    "        return sorted_g_list\n",
    "    \n",
    "    def parse(self,sentence,grammars):\n",
    "        \"\"\"\n",
    "        parse sentence with the CYK algorithm\n",
    "        \"\"\"\n",
    "        s_arr = sentence.lower().split(\" \")\n",
    "        s_len = len(s_arr)\n",
    "        d = {}\n",
    "        for level in range(s_len):\n",
    "            cur_gap = level\n",
    "            for span_start in range(s_len-level):\n",
    "                span_stop = span_start + cur_gap\n",
    "                cur_span = str(span_start)+\"-\"+str(span_stop)\n",
    "                d[cur_span] = []\n",
    "                if(level == 0): \n",
    "                    # 加入所有的lexical rule\n",
    "                    for g in grammars:\n",
    "                        if len(g[1]) == 1 and s_arr[span_start] == g[1][0]:\n",
    "                            d[cur_span].append([g[0],g[2],s_arr[span_start],cur_span,g])\n",
    "                else:\n",
    "                    for span_split in range(span_start,span_stop):\n",
    "                        left_span = str(span_start)+\"-\"+str(span_split)\n",
    "                        right_span = str(span_split+1)+\"-\"+str(span_stop)\n",
    "                        if left_span in d.keys() and right_span in d.keys():\n",
    "                            left_index = 0\n",
    "                            right_index = 0\n",
    "                            for left_child in d[left_span]:\n",
    "                                for right_child in d[right_span]:\n",
    "                                    for g in grammars:\n",
    "                                        if len(g[1]) == 2 and left_child[0] == g[1][0] and right_child[0] == g[1][1]:\n",
    "                                            inner_prob = g[2]*left_child[1]*right_child[1] # hyperedge * left * right\n",
    "                                            d[cur_span].append([g[0],inner_prob,[left_span,left_index],[right_span,right_index],cur_span,g])\n",
    "                                    right_index += 1\n",
    "                                left_index += 1\n",
    "        return d\n",
    "    \n",
    "    def find_max_parser(self,sentence,d):\n",
    "        \"\"\"\n",
    "        find the most likely tree\n",
    "        \"\"\"\n",
    "        s_len = len(sentence.lower().split(\" \"))\n",
    "        final_span = \"0-\"+str(s_len-1)\n",
    "        root = [0,0]\n",
    "        for root_candidate in d[final_span]:\n",
    "            if root_candidate[1] > root[1]:\n",
    "                root = root_candidate\n",
    "        prob = root[1]\n",
    "        parser_nodes = [root]\n",
    "        to_expand = [root]\n",
    "        while len(to_expand) != 0:\n",
    "            cur_node = to_expand.pop(0)\n",
    "            cur_node_index = parser_nodes.index(cur_node)\n",
    "            if type(cur_node[3]) is not str:\n",
    "                left = d[cur_node[2][0]][cur_node[2][1]]\n",
    "                right = d[cur_node[3][0]][cur_node[3][1]]\n",
    "                to_expand.insert(0,right)\n",
    "                to_expand.insert(0,left)\n",
    "                parser_nodes.insert(cur_node_index ,\"(\")\n",
    "                parser_nodes.insert(cur_node_index + 2, left )\n",
    "                parser_nodes.insert(cur_node_index + 3, right)\n",
    "                parser_nodes.insert(cur_node_index + 4, \")\")\n",
    "            else:\n",
    "                parser_nodes.insert(cur_node_index , \"(\")\n",
    "                parser_nodes.insert(cur_node_index + 2, cur_node[2])\n",
    "                parser_nodes.insert(cur_node_index + 3, \")\")\n",
    "        parser_str_arr = []\n",
    "        for node in parser_nodes:\n",
    "            if type(node) is str:\n",
    "                parser_str_arr.append(node)\n",
    "            else:\n",
    "                parser_str_arr.append(node[0])\n",
    "            parser_str_arr.append(\" \")\n",
    "        parser_str = \"\".join(parser_str_arr)\n",
    "        return parser_str,parser_nodes,prob\n",
    "    \n",
    "    def inner(self,sentence,d):\n",
    "        \"\"\"\n",
    "        inside probability\n",
    "        \"\"\"\n",
    "        inner = {}\n",
    "        s_len = len(sentence.lower().split(\" \"))\n",
    "        for level in range(s_len):\n",
    "            for span_start in range(s_len - level):\n",
    "                span_stop = span_start + level\n",
    "                cur_span = str(span_start) + \"-\" + str(span_stop)\n",
    "                cur_span_nodes = d[cur_span]\n",
    "                if len(cur_span_nodes) != 0:\n",
    "                    inner[cur_span] = {}\n",
    "                    for node in cur_span_nodes:\n",
    "                        if node[0] not in inner[cur_span].keys():\n",
    "                            inner[cur_span][node[0]] = node[1]\n",
    "                        else:\n",
    "                            inner[cur_span][node[0]] += node[1]\n",
    "        return inner\n",
    "\n",
    "    def outer(self,sentence,d_s):\n",
    "        \"\"\"\n",
    "        outside probability\n",
    "        \"\"\"\n",
    "        outer = {}\n",
    "        s_len = len(sentence.lower().split(\" \"))\n",
    "        for level in range(s_len-1,-1,-1):\n",
    "            for span_start in range(s_len - level):\n",
    "                span_stop = span_start + level\n",
    "                cur_span = str(span_start) + \"-\" + str(span_stop)\n",
    "                cur_span_nodes = d_s[cur_span]\n",
    "                if len(cur_span_nodes) != 0:\n",
    "                    outer[cur_span] = {}\n",
    "                    if level == s_len-1:\n",
    "                        outer[cur_span][\"S\"]=1.0\n",
    "                        for index in range(len(cur_span_nodes)):\n",
    "                            d_s[cur_span][index].append(1.0)\n",
    "                    else:\n",
    "                        for node in cur_span_nodes:\n",
    "                            if type(node[-1]) is not list:\n",
    "                                if node[0] not in outer[cur_span].keys():\n",
    "                                    outer[cur_span][node[0]] = node[-1]\n",
    "                                else:\n",
    "                                    outer[cur_span][node[0]] += node[-1]\n",
    "                    if level != 0:\n",
    "                        for node in cur_span_nodes:\n",
    "                            if type(node[-1]) is not list:\n",
    "                                cur_tag = node[0]\n",
    "                                left_child = node[2]\n",
    "                                left_child_in = d_s[left_child[0]][left_child[1]][1]\n",
    "                                right_child = node[3]\n",
    "                                right_child_in = d_s[right_child[0]][right_child[1]][1]\n",
    "                                g = node[5]\n",
    "                                parent_out_prob = outer[cur_span][cur_tag]\n",
    "                                g_prob = g[2]\n",
    "                                left_child_out = parent_out_prob * left_child_in * g_prob\n",
    "                                right_child_out = parent_out_prob * right_child_in * g_prob\n",
    "                                d_s[left_child[0]][left_child[1]].append(left_child_out)\n",
    "                                d_s[right_child[0]][right_child[1]].append(right_child_out)\n",
    "        return outer,d_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a PCFG\n",
    "根据题目给出的两棵句法树写出PCFG的规则，包括phrasal rule和lexical rule。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcfg = PCFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_grammar = pcfg.count_grammer(train_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出文件modelFile\n",
    "\n",
    "先输出终结符的grammar 再输出非终结符的grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_file = \"data/modelFile\"\n",
    "with open(model_file,\"w\") as f:\n",
    "    for g in sorted_grammar:\n",
    "        # tag # word # prob\n",
    "        if len(g[1]) == 1:\n",
    "            f.write(g[0]+\" # \"+\" \".join(g[1])+\" # \"+str(g[2])+\"\\n\")\n",
    "    for g in sorted_grammar:\n",
    "        # LHS # RHS # prob\n",
    "        if len(g[1]) > 1:\n",
    "            f.write(g[0]+\" # \"+\" \".join(g[1])+\" # \"+str(g[2])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find the most likely parse\n",
    "a boy with a telescope saw a girl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = \"a boy with a telescope saw a girl\"\n",
    "# ( S ( NP ( NP ( DT a ) ( NN boy ) ) ( PP ( IN with ) ( NP ( DT a ) ( NN telescope ) ) ) ) ( VP ( VBD saw ) ( NP ( DT a ) ( NN girl ) ) ) ) '\n",
    "d_s = pcfg.parse(s,sorted_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_parser, parser_nodes, prob = pcfg.find_max_parser(s,d_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate the inside and outside log probabilities for all spans and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcfg = PCFG()\n",
    "d_inner = pcfg.inner(s,d_s)\n",
    "d_outer, d_s = pcfg.outer(s,d_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出文件parseFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def log(prob):\n",
    "    return \"%.6f\" % np.log10(prob)\n",
    "\n",
    "def output(s,prob,s_parser,d_inner,d_outer,model_file):\n",
    "    with open (model_file,\"w\") as f:\n",
    "        # the most likely parse in the bracketed form\n",
    "        f.write(s_parser + \"\\n\")\n",
    "        # the highest probability\n",
    "        f.write(log(prob) + \"\\n\")\n",
    "        s_n = len(s.split(\" \"))\n",
    "        for level in range(s_n):\n",
    "            for span_start in range(s_n - level):  # # 0,1,...,s_n-level-1\n",
    "                span_stop = span_start + level\n",
    "                cur_span = str(span_start) + \"-\" + str(span_stop)  # 0-1\"\n",
    "                if cur_span in d_inner.keys():\n",
    "                    for node in d_inner[cur_span].items():\n",
    "                        cur_tag = node[0]\n",
    "                        inprob = d_inner[cur_span][cur_tag]\n",
    "                        if cur_tag not in d_outer[cur_span].keys():\n",
    "                            outprob = 0.0\n",
    "                        else:\n",
    "                            outprob = d_outer[cur_span][cur_tag]\n",
    "                        # Nj # p # q # betaj(p,q) # alphaj(p,q)\n",
    "                        f.write(cur_tag +\" # \" + str(span_start) +\" # \" + str(span_stop) +\" # \" + log(inprob) +\" # \" + log(outprob) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser_file = \"data/parseFile\"\n",
    "output(s,prob,s_parser,d_inner,d_outer,parser_file)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
