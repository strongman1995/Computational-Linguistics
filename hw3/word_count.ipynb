{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import chardet  \n",
    "path='corpus/'\n",
    "def paths(path):\n",
    "    path_collection=[]\n",
    "    for dirpath,dirnames,filenames in os.walk(path):\n",
    "        for file in filenames:\n",
    "            fullpath=os.path.join(dirpath,file)\n",
    "            path_collection.append(fullpath)\n",
    "    return path_collection\n",
    "\n",
    "words = [] #文档中出现的所有词\n",
    "for i,file in enumerate(paths(path)):\n",
    "# for i,file in enumerate([\"corpus/01060108.txt\"]):\n",
    "    with open(file) as somefile:\n",
    "        for line in somefile:\n",
    "            linestr = line.strip().decode(\"gbk\")\n",
    "            for w in linestr.split('  '):\n",
    "                tmp = w.split('/')[0]\n",
    "                if len(tmp)>1 and tmp[0] == '[':\n",
    "                    tmp = tmp[1:]\n",
    "                words.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separator = [u'。',u'？',u'！',u'：',u'；',u'?',u'!',u':',u';']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bigram\n",
    "sentences = [] #文档中的所有句子，加了<BOS>和<EOS>\n",
    "for i,file in enumerate(paths(path)):\n",
    "# for i,file in enumerate([\"corpus/01060108.txt\"]):\n",
    "    with open(file) as somefile:\n",
    "        for line in somefile:\n",
    "            linestr = line.strip().decode(\"gbk\")\n",
    "            line_list = linestr.split('  ')\n",
    "            sentence = [u'<BOS>']\n",
    "            for w in line_list:\n",
    "                tmp = w.split('/')[0]\n",
    "                if len(tmp)>1 and tmp[0] == '[':\n",
    "                    tmp = tmp[1:]\n",
    "                if tmp in separator:\n",
    "                    sentence.append(u'<EOS>')\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = [u'<BOS>']\n",
    "                else:\n",
    "                    sentence.append(tmp)\n",
    "            if line_list[-1] not in separator and len(sentence) > 1:\n",
    "                sentence.append(u'<EOS>')\n",
    "                sentences.append(sentence)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50609"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> 《 十日谈 》 抄本 重见天日 十八 幅 插图 蔚为大观 <EOS>\n",
      "<BOS> 据 新华社 罗马 １月 ９日 电 （ 记者 刘 儒庭 ） 意大利 文艺复兴 时期 著名 作家 薄伽丘 （ １３１３年 — １３７５年 ） 的 名著 《 十日谈 》 手抄本 和 其中 的 １８ 幅 插图 最近 被 发现 ， 这 对 研究 这 位 作家 和 这部 世界 文学 名著 具有 重要 意义 <EOS>\n",
      "<BOS> 意大利 研究 《 十日谈 》 的 著名 专家 维·布朗卡 ９日 在 罗马 林琴 科学院 介绍 了 他 的 这 一 发现 <EOS>\n",
      "<BOS> 他 说 ， 这 一 手抄本 是 在 巴黎 国家 档案馆 发现 的 ， 是 薄伽丘 ４０ 岁 之前 创作 的 ， 同 他 后来 在 １３７０年 左右 重 写 的 版本 在 语言 和 风格 上 明显 不同 <EOS>\n",
      "<BOS> 薄伽丘 早年 写 的 这 一 版本 是 严格 按 写作 规范 写 的 ， 后来 的 版本 写作 风格 则 更 自由 一些 <EOS>\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences[:5]:\n",
    "    print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<BOS>',\n",
       " u'\\u300a',\n",
       " u'\\u5341\\u65e5\\u8c08',\n",
       " u'\\u300b',\n",
       " u'\\u6284\\u672c',\n",
       " u'\\u91cd\\u89c1\\u5929\\u65e5',\n",
       " u'\\u5341\\u516b',\n",
       " u'\\u5e45',\n",
       " u'\\u63d2\\u56fe',\n",
       " u'\\u851a\\u4e3a\\u5927\\u89c2',\n",
       " u'<EOS>']"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_sentences = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_files = len(paths(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档数： 3148\n",
      "句子数： 50609\n",
      "词数： 1120721\n"
     ]
    }
   ],
   "source": [
    "print \"文档数：\",total_files\n",
    "print \"句子数：\",total_sentences\n",
    "print \"词数：\",total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordcount(words):\n",
    "    # 文章字符串前期处理\n",
    "    strl_ist = words\n",
    "    count_dict = {}\n",
    "    # 如果字典里有该单词则加1，否则添加入字典\n",
    "    for str in strl_ist:\n",
    "        if str in count_dict.keys():\n",
    "            count_dict[str] = count_dict[str] + 1\n",
    "        else:\n",
    "            count_dict[str] = 1\n",
    "    #按照词频从高到低排列\n",
    "    count_list=sorted(count_dict.iteritems(),key=lambda x:x[1],reverse=True)\n",
    "    return count_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_list = wordcount(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = open('1.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('1.txt','w') as f1:\n",
    "    for i,c in enumerate(count_list):\n",
    "        f1.write((c[0]+\"\\t\"+str(c[1])+\"\\n\").encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t， \t74921\n",
      "1 \t的 \t54476\n",
      "2 \t。 \t35983\n",
      "3 \t、 \t23116\n",
      "4 \t在 \t12022\n",
      "5 \t了 \t11557\n",
      "6 \t和 \t10919\n",
      "7 \t是 \t9819\n",
      "8 \t“ \t7970\n",
      "9 \t” \t7943\n"
     ]
    }
   ],
   "source": [
    "for i,c in enumerate(count_list[:10]):\n",
    "    print i,\"\\t\",c[0],\"\\t\",c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in count_list:\n",
    "    count_dict[c[0]] = c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict[u\"扶贫\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni.unigram[u'扶贫']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('freq.txt','w') as f1:\n",
    "    for i,c in enumerate(count_list):\n",
    "        f1.write((str(i+1)+\"\\t\"+c[0]+\"\\t\"+str(c[1])+\"\\n\").encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('freq.txt','w') as f1:\n",
    "    for i,c in enumerate(count_list):\n",
    "        f1.write((str(i+1)+\"\\t\"+c[0]+\"\\t\"+str(c[1])+\"\\n\").encode('gbk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NGram(object):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        # n is the order of n-gram language model\n",
    "        self.n = n\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self.uniSize = 0 #不同unigram个数\n",
    "        self.biSize = 0 #不同bigram个数\n",
    "\n",
    "    # scan a sentence, extract the ngram and update their\n",
    "    # frequence.\n",
    "    #\n",
    "    # @param    sentence    list{str}\n",
    "    # @return   none\n",
    "    def scan(self, sentence):\n",
    "        # file your code here\n",
    "        for line in sentence:\n",
    "            self.ngram(line)\n",
    "            \n",
    "    # caluclate the ngram of the words\n",
    "    #\n",
    "    # @param    words       list{str}\n",
    "    # @return   none\n",
    "    def ngram(self, words):\n",
    "        # unigram\n",
    "        if self.n == 1:\n",
    "            for word in words:\n",
    "                if word not in self.unigram:\n",
    "                    self.unigram[word] = 1\n",
    "                    self.uniSize = self.uniSize +1\n",
    "                else:\n",
    "                    self.unigram[word] = self.unigram[word] + 1\n",
    "\n",
    "        # bigram\n",
    "        if self.n == 2:\n",
    "            pre_word = words[0]\n",
    "            for w in words[1:]:\n",
    "                stri = pre_word + \" \" + w\n",
    "                if stri not in self.bigram:\n",
    "                    self.bigram[stri] = 1\n",
    "                    self.biSize = self.biSize +1\n",
    "                else:\n",
    "                    self.bigram[stri] = self.bigram[stri] + 1\n",
    "                pre_word = w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uni = NGram(1)\n",
    "bi = NGram(2)\n",
    "uni.scan(sentences)\n",
    "bi.scan(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55411 457709\n"
     ]
    }
   ],
   "source": [
    "print uni.uniSize, bi.biSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1 = u\"扶贫 开发 工作 取得 很 大 成绩\"\n",
    "sentence2 = u\"扶贫 开发 工作 得到 很 大 成绩\"\n",
    "sentence1a = u\"<BOS> 扶贫 开发 工作 取得 很 大 成绩\"\n",
    "sentence2a = u\"<BOS> 扶贫 开发 工作 得到 很 大 成绩\"\n",
    "sentence1b = u\"<BOS> 扶贫 开发 工作 取得 很 大 成绩 <EOS>\"\n",
    "sentence2b = u\"<BOS> 扶贫 开发 工作 得到 很 大 成绩 <EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扶贫 363\n",
      "开发 484\n",
      "工作 2412\n",
      "取得 582\n",
      "很 912\n",
      "大 2244\n",
      "成绩 230\n"
     ]
    }
   ],
   "source": [
    "for w in senstence1.split(' '):\n",
    "    print w,uni.unigram[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 第一行输出句子1的unigram句子概率，并分别输出每个unigram的概率。\n",
    "2. 第二行输出句子1的bigram句子概率，并分别输出每个bigram的条件概率。\n",
    "3. 第三行输出句子1a的bigram句子概率，并补充输出与<BOS>相关的bigram条件概率。\n",
    "4. 第四行输出句子1b的bigram句子概率，并补充输出与<BOS>和<EOS>相关的bigram条件概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigram_prob(unigram):\n",
    "    return float(uni.unigram[unigram])/float(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigram_sentence_prob(sentence):\n",
    "    word_list = sentence.split(' ')\n",
    "    if word_list[0] == u'<BOS>':\n",
    "        word_list = word_list[1:]\n",
    "    if word_list[-1] == u'<EOS>':\n",
    "        word_list = word_list[:-1]\n",
    "    uni_prob = float(1)\n",
    "    for w in word_list:\n",
    "        w_prob = unigram_prob(w)\n",
    "        uni_prob = uni_prob * w_prob\n",
    "    return uni_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(<s> I want english food </s>) = P(I|<s>) ×  P(want|I) × P(english|want) ×  P(food|english)  ×  P(</s>|food) =  .000031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generateBigram(sentence):\n",
    "    word_list = sentence.split(' ')\n",
    "    bigram_list = []\n",
    "    pre_word = word_list[0]\n",
    "    for w in word_list[1:]:\n",
    "        bigram_list.append(pre_word + \" \" + w)\n",
    "        pre_word = w \n",
    "    return bigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_prob(bigram):\n",
    "    bi_list = bigram.split(' ')\n",
    "    pre_word = bi_list[0]\n",
    "    pre_word_count = uni.unigram[pre_word]\n",
    "    if bigram not in bi.bigram:\n",
    "        bi.bigram[bigram] = 0\n",
    "    bigram_count = bi.bigram[bigram]\n",
    "    return float(bigram_count)/float(pre_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_sentence_prob(sentence):\n",
    "    word_list = sentence.split(' ')\n",
    "    bi_list = generateBigram(sentence)\n",
    "    bi_sent_prob = 0\n",
    "    if word_list[0] == u'<BOS>':\n",
    "        bi_sent_prob = 1\n",
    "    else:\n",
    "        bi_sent_prob = unigram_prob(word_list[0])\n",
    "    for b in bi_list:\n",
    "        bi_prob = bigram_prob(b)\n",
    "        bi_sent_prob = bi_sent_prob * bi_prob\n",
    "    return bi_sent_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.227755085591562e-23"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentence_prob(sentence1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.350434101246826e-23"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_sentence_prob(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatF(number):\n",
    "    return \"%.6f\" % np.log10(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 行内以tab分隔，概率输出取log（10为底）结果，小数点后保留6位。\n",
    "with open(\"3.txt\",\"wb\") as f:\n",
    "    #第一行输出句子1的unigram句子概率，并分别输出每个unigram的概率。\n",
    "    f.write(formatF(unigram_sentence_prob(sentence1))+\"\\t\")\n",
    "    for w in sentence1.split(' '):\n",
    "        f.write(formatF(unigram_prob(w))+\"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    #第二行输出句子1的bigram句子概率，并分别输出每个bigram的条件概率。\n",
    "    f.write(formatF(bigram_sentence_prob(sentence1))+\"\\t\")\n",
    "    for b in generateBigram(sentence1):\n",
    "        f.write(formatF(bigram_prob(b))+\"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    #第三行输出句子1a的bigram句子概率，并补充输出与<BOS>相关的bigram条件概率。\n",
    "    f.write(formatF(bigram_sentence_prob(sentence1a))+\"\\t\")\n",
    "    for b in generateBigram(sentence1a):\n",
    "        f.write(formatF(bigram_prob(b))+\"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    #第四行输出句子1b的bigram句子概率，并补充输出与<BOS>和<EOS>相关的bigram条件概率。\n",
    "    f.write(formatF(bigram_sentence_prob(sentence1b))+\"\\t\")\n",
    "    for b in generateBigram(sentence1b):\n",
    "        f.write(formatF(bigram_prob(b))+\"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    #第一行输出句子2的unigram句子概率，并分别输出每个unigram的概率。\n",
    "    f.write(formatF(unigram_sentence_prob(sentence2))+\"\\t\")\n",
    "    for w in sentence2.split(' '):\n",
    "        f.write(formatF(unigram_prob(w))+\"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    #第二行输出句子2的bigram句子概率，并分别输出每个bigram的条件概率。\n",
    "    f.write(formatF(bigram_sentence_prob(sentence2))+\"\\t\")\n",
    "    for b in generateBigram(sentence2):\n",
    "        f.write(formatF(bigram_prob(b))+\"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    #第三行输出句子2a的bigram句子概率，并补充输出与<BOS>相关的bigram条件概率。\n",
    "    f.write(formatF(bigram_sentence_prob(sentence2a))+\"\\t\")\n",
    "    for b in generateBigram(sentence2a):\n",
    "        f.write(formatF(bigram_prob(b))+\"\\t\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    #第四行输出句子2b的bigram句子概率，并补充输出与<BOS>和<EOS>相关的bigram条件概率。\n",
    "    f.write(formatF(bigram_sentence_prob(sentence2b))+\"\\t\")\n",
    "    for b in generateBigram(sentence2b):\n",
    "        f.write(formatF(bigram_prob(b))+\"\\t\")\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55411"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni.uniSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457709"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi.biSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
